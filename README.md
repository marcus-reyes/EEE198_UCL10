# EEE 198: Simulated Annealing for Model Compression

## Authors

This codebase was developed by Jeff Sanchez and Marcus Reyes for during 
their senior-year project; UCL, UP-Diliman 2020.


## Background

This library is the implementation of our thesis project, titled
"thesis-title-here." Taking inspiration from recent works on lottery tickets
[1,2], Model Compression [3], and Early Pruning [4], we wanted to automate 
the process of finding high performing and high sparsity subnetworks (winning 
tickets), without relying on already trained networks. This form of compression
allows for memory and energy savings in training neural networks. After our 
initial attempt of using an [RL agent](github.com/prokorpio/everything_190) to 
search for these subnetworks, we resolved to using a simpler approach based on 
the heuristic search algorithm, Simulated Annealing (SA). 

SA works by iteratively trying out various masks and selects the one that
yields the best estimated performance. That is, given a network with full
parameters, the algorithm will heuristically traverse a search space of masks 
(which represents parameters to be pruned or kept), on each step accepting a 
new candidate mask if it improves performance, otherwise, will accept it 
with probability based on how bad it degrades performance and a global
temperature variable that makes it less and less likely to accept bad masks
towards the end of the process. 

To evaluate the performance of SA, we run the following experiments:

1. SA on CNN
    1. Mask Search on CNN
        1. Randomly initialize the neural network (initialization w_{init})
        2. Train for K epochs (K $\in$ [0,90])
        3. Run SA search
        4. Save best mask found

    2. Actual Pruning CNN
        1. Load SA-found mask
        2. Load masks generated using other criterion
        3. Apply different masks on multiple network intances (with similar
           initializations w_{init})
        4. Prune: delete filters based on masks
        5. Save pruned networks

    3. Training Pruned CNN
        1. Load pruned networks 
        2. Train networks for 90 epochs on CIFAR-10
        3. Save performance logs

2. SA on MLP  
    Since only preliminary experiments and analysis were done on MLP, we only
    implemented SA search and masked-pruning (no actual deletion of parameters).
    Similar steps are followed as above, but without actual pruning. 

Additional analysis were performed on the final SA mask performances through 
the following experiments:

1. Orthogonality scores and training loss plots on MLP. To shed light on the
   disconnect between high untrained performance yet subpar final (trained)
   performance.
2. Mask similarity matrices on MLP and CNN. To examine relationships between
   masks generated by SA and those by other criterion.
3. Testing the effect of using larger validation set for SA mask performance
   estimation.


## Summary of Results

### MLP Experiments
1. Sample plot of SA search  
    <insert sample search plot>
    The plot above shows in red the estimated performance of masks tried by SA
    throughout its search process. This specific run prunes 70% of the weights,
    and the estimated performance is taken with only n-epoch training. Shown in
    blue is the relative hamming distance of masks tested, experimentally
    tuned, and serves to increasingly limit the neighborhood of the search
    space.  

2. Criterion Comparison  
    <insert table 5.5>  
    We observe three things from the table above: (1) Across sparsities, SA and 
    Random are consistently the bottom two compared to the rest of the other 
    criteria. (2) The achieved optimized accuracy of SA on the validation set 
    is still lower than that of Deconst<sup id="a1">[1](#f1)</sup>. (3) SA has 
    significant discrepancy between validation accuracy (maximized accuracy on a 
    validation set used in SA search) and rewind accuracy (test accuracy of the 
    final SA mask applied at initialization). The following empirical analyses 
    shed more light in these observations.  
    
3. Trainability  
    <insert OS scores>  
    By the virtue of having high orthogonality scores<sup id="a2">[2](#f2)</sup> 
    and slow-moving training loss plots, we concluded that both SA and Random 
    criterion  exhibit  a  more  hampered  training  as  compared  to  other  
    criteria  which  results in the two attaining sub par final performances.
    Additionally, although we resolved not to extend the OS formula for
    convolutional layers, the training loss plots for CNN showed similar
    trends. Specifically, Mag criterion more rapidly plummets as compared to SA
    and Rand, however its rapid downward slope decreases as sparsity is
    increased.  

4. Mask similarities  
    By looking at the cosine similarities among different masks obtained by SA,
    derived from other criterions, and random masks (all represented as vectors
    and compared in groups, accordingly), we concluded that it is only when the 
    objective function used in SA (accuracy of masked network) is based on 
    partially trained accuracy that this objective function is better able to 
    guide the search process towards better regions in the search space. In
    other words, the SA search effectiveness is limited by the information 
    available from the objective function. CNN data agrees with this observation.  

5. Discrepancy on validation and test accuracies  
    Similar to other low-fidelity performance estimation [6], the 
    use of a small subset of training data to estimate the mask performance 
    proved to cause overfitting on this small validation set. This is
    empirically proven in CNN experiments.   

### CNN Experiments
1. Sample plot of SA search  
    <insert sample search plot>
    Similar as the plot for CNN; this one shows SA on CNN, selecting
    only 70% of filters. 

2. Criterion Comparison  
    <insert table 5.8 from paper>  
    The  data  here  indicates  three  things:   (1) By  itself  SA  performs  
    relatively  well with only around 2% and 6% accuracy drop at 70% and 80% 
    filter sparsity.(2) At the extreme sparsities SA outperforms Mag Rewind 
    and performs on par or better than Mag Sign Rewind<sup id="a2">[3](#f3)</sup>; 
    (3) Increasing the epoch k wherein the SA algorithm is applied causes a 
    corresponding consistent increase in the converged accuracy of SA.

3. Mask Similarities  
    Similar to the MLP findings, we find that increasing partial-training k 
    results to the search exploring more focused regions in the search space 
    which itself are closer to where other good masks are (masks from 
    different criterions).

4. Discrepancy on validation and test accuracies 
    To test if the validation set limits the performance of the SA search, we
    experimented on using larger validation set sizes and found that indeed
    this mehotd of low-fidelity estimation results to a strong bias towards the
    small validation data.



## Getting Started

1. Run `setup.py` to install library dependencies. (I'll [add](https://python-packaging.readthedocs.io/en/latest/dependencies.html)) 

2. Modify paths... (if necessary)

3. Run ...


## Code Files

### Main components
1. main_SA.py
2. environment.py
3. utilities.py
4. trainer.py and train_actual_subnet.py
5. MLP_experiments/...

### Analysis Scripts
1. mask_similarities.py

### Paths and Log Folders

## Datasets
1. [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) for CNN experiments.
2. [MNIST](http://yann.lecun.com/exdb/mnist/) for MLP experiments.

## Footnotes
<sup id="f1">1</sup> Mask generated using Magnitude-Sign (MagSign), criterion as 
defined in Deconstructing Lottery Ticket by Zhou et al [2]. [↩](#a1)  
<sup id="f2">2</sup>higher OS means less propagation of learning signals [5][↩](#a2)  
<sup id="f3">3</sup>extreme filter sparsity of 80%, Mag Rewind consistently prunes an
entire layer and thus has no output.[↩](#a3)   


## References
[1] [Lottery Ticket Hypothesis](https://github.com/google-research/lottery-ticket-hypothesis)  
[2] [Deconstructing Lottery Ticket](https://github.com/uber-research/deconstructing-lottery-tickets)  
[3] [AutoML for Model Compression](https://github.com/mit-han-lab/amc-models)  
[4] [SNIP](https://github.com/namhoonlee/snip-public)  
[5] [SNIP-2](https://arxiv.org/abs/1906.06307)  
[6] [NAS Survey](https://arxiv.org/abs/1808.05377)

